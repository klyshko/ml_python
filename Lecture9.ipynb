{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture 9: Introduction to Machine Learning in Python\n",
    "---\n",
    "\n",
    "## 1. Introduction <a id='l_overview'></a>\n",
    "\n",
    "This lecture was created as part of a CPS Teaching Fellowship. We are introducing a novel approach to study advanced scientific programming. The goal of today's lecture is to present unsupervised Machine Learning. We will learn about the most typical machine learning problems, such as dimensionality reduction, and how to approach these using the Python programmming language. These are the important concepts that we will cover:\n",
    "\n",
    "- [Machine Learning](#l_ml)\n",
    "- [Data sets](#l_ds)\n",
    "- [Dimensionality reduction](#l_dr) \n",
    "- [Principal Component Analysis (PCA)](#l_pca)\n",
    "- [Multidimensional Scaling (MDS)](#l_mds)\n",
    "- [Other dimensionality reduction techniques](#l_other)\n",
    "\n",
    "\n",
    "## 2. Machine Learning <a id='l_ml'></a>\n",
    "\n",
    "Below is the outline of the field with specific algorithms:\n",
    "\n",
    "1. **Unsupervised Learning** - there is no correct input/output pair \n",
    "    - *Clustering*\n",
    "        - K-Means\n",
    "        - Hierarchical\n",
    "        - Spectral\n",
    "    - *Dimensionality reduction*\n",
    "        - Principal Components Analysis (PCA)\n",
    "        - Multidimensional Scaling (MDS)\n",
    "        - Stochastic Neighbour Embedding (t-SNE)\n",
    "        - Uniform Manifold Approximation and Projection (UMAP)\n",
    "        \n",
    "        \n",
    "2. **Supervised Learning** - there is a correct input/output pair\n",
    "    - *Regression*\n",
    "        - Curve fitting\n",
    "        - Linear regression \n",
    "    - *Classification*\n",
    "        - Linear Classifiers (Support Vector Machines, Logistic regression)\n",
    "        - Decision Trees\n",
    "        - Neural Networks\n",
    "        \n",
    "        \n",
    "3. **Reinforcement Learning** - is an area concerned with how software agents have to take actions in an environment so as to maximize some cumulative reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating data sets\n",
    "\n",
    "Setup:\n",
    "- Suppose one has $p$ samples of N-dimensional data points, $x_i\\in\\mathbb{R}^N$\n",
    "- Store these samples columnwise as $X\\in\\mathbb{R}^{p\\,\\times\\,N}$\n",
    "- We call this the original data matrix, or simply the data\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the data space (high dim)\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the latent space (low dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate linear data with noise ( 2-dimensional data set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-poster')\n",
    "\n",
    "raw_data_x = np.random.uniform(0,10, size=(100,))\n",
    "raw_data_y = 0.5 * raw_data_x + np.random.normal(0,1,len(raw_data_x))\n",
    "\n",
    "X_2d = np.empty((100, 2))\n",
    "X_2d[:,0] = raw_data_x\n",
    "X_2d[:,1] = raw_data_y\n",
    "\n",
    "#plt.figure(figsize=(12,8))\n",
    "plt.scatter(X_2d[:,0], X_2d[:,1])\n",
    "plt.xlim(-1, 11)\n",
    "plt.ylim(-2, 7.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look how the data looks like (first 10 points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some high-dimensional data from the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. \n",
    "![](pics/mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take only 1000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_test[:1000]\n",
    "Y = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[567], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to convert each data point (picture with a handwritten digit) to a vector which dimensionality is 28x28 = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(1000, 784)\n",
    "X[567].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary** - we have two data sets:\n",
    "- Two-dimensional data set with 100 points\n",
    "- 768-dimensional data set with 1000 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality reduction <a id='l_dr'></a>\n",
    "Dimensionality reduction is the process of reducing the number of dimensions under consideration by obtaining a set of principal variables. You can select a subset of original variables, or find a linear or nonlinear combination of features, or make a projection to lower dimensions. \n",
    "\n",
    "![](pics/dr.png)\n",
    "\n",
    "\n",
    "Methods:\n",
    "- **Principal Components Analysis (PCA)** - linear method to extract dimensions with the highest variance\n",
    "- **Multidimensional Scaling (MDS)** - nonlinear method to project in lower dimensions by saving pairwise distances\n",
    "- **Stochastic Neighbour Embedding (t-SNE)** - making an embedding in lower dimensions by conserving distribution of distances \n",
    "- **Uniform Manifold Approximation and Projection (UMAP)** - projecting the data on manifold into fewer dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Principal Component Analysis <a id='l_dr'></a>\n",
    "\n",
    "**Math**:\n",
    "\n",
    "- PCA goal: Find orthogonal transformation $W$ of centered data $X_c$ (i.e. $Y=WX_c$) such that variance along subsequent components is maximized (i.e. most variance along first, etc.); note $X_c$ is $p \\times N$, $W$ is $N \\times N$, $Y$ is $p \\times N$, principal components are the columns of $W$\n",
    "- principal components of $X_c$ are typically found via eigendecomposition of covariance matrix $X_c^T X_c$ \n",
    "- the PCA embedding is $Y=U^T X_c$, where $U$ stores columnwise eigenvectors of $X_c^T X_c$ in decreasing order (by eigenvalue)\n",
    "\n",
    "#### Compute principle components via eigenvectors of covariance matrix\n",
    "\n",
    "1. Center data, i.e. first subtract the mean from the data\n",
    "2. Compute the covariance matrix\n",
    "3. Compute eigenvectors and order them in terms of decreasing eigenvalues\n",
    "4. Transform the data using these eigenvectors\n",
    "5. Compare to library PCA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data set # 1:\n",
    "\n",
    "# step 1\n",
    "X_2d_centered = X_2d - np.mean(X_2d, axis=0)\n",
    "plt.scatter(X_2d_centered[:,0], X_2d_centered[:,1])\n",
    "plt.ylim((-4,4))\n",
    "plt.xlim((-6,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is now centered on the origin.\n",
    "Now compute the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.\n",
    "Cov = np.dot(np.transpose(X_2d_centered), X_2d_centered)\n",
    "print(\"Covariance matrix:\")\n",
    "print(Cov)\n",
    "\n",
    "#Step 3\n",
    "Eigvals, W = np.linalg.eig(Cov)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(Eigvals)\n",
    "print(\"\\nEigenvectors (columns)\")\n",
    "print(W)\n",
    "print(\"\\nCheck that eigenvectors are orthogonal (<w1,w2>=0):\")\n",
    "print(np.dot(W[:,0],W[:,1]))\n",
    "\n",
    "print('\\nVarience in the first principal component: {}, varience in the second principal component: {}'.format(Eigvals[0]/np.sum(Eigvals), Eigvals[1]/np.sum(Eigvals) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the eigenvectors in comparison to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_2d_centered[:,0], X_2d_centered[:,1])\n",
    "plt.plot([0, W[0][1]*3],[0, W[1][1]*3],'r',linewidth=4)\n",
    "plt.plot([0, W[0][0]*5],[0, W[1][0]*5],'g',linewidth=4)\n",
    "plt.ylim((-6,6))\n",
    "plt.xlim((-8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the transformation to the data and plot the data in the new space. We flip the matrix W and corresponding eigenvalues so that they are ordered the same way as in the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4\n",
    "#W = np.fliplr(W)\n",
    "Eigvals = Eigvals[::-1]\n",
    "X_2d_transformed = np.dot(X_2d_centered, W)\n",
    "plt.scatter(X_2d_transformed[:,0], X_2d_transformed[:,1])\n",
    "plt.ylim((-6,6))\n",
    "plt.xlim((-8,8))\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our naive implementation to the PCA implementation from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "skl_PCA = PCA(n_components = 2).fit(X_2d) ## fit the data to receive eigenvectors of covariance matrix\n",
    "skl_X_2d_transformed = skl_PCA.transform(X_2d) ## apply a transformation\n",
    "\n",
    "plt.scatter(skl_X_2d_transformed[:,0], skl_X_2d_transformed[:,1])\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.ylim((-6,6))\n",
    "plt.xlim((-8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks identical (up to a left-right or up-down flip)! \n",
    "\n",
    "We will now truncate the data to one dimension and see how it looks. It's called a simple PCA dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skl_PCA.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that >90% of variance is in the first principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(skl_X_2d_transformed[:,0] , np.zeros(shape=skl_X_2d_transformed[:,0].shape))\n",
    "plt.ylim((-6,6))\n",
    "plt.xlim((-8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1\n",
    "\n",
    "Perform principal component analysis on the 1000 points of MNIST data set. It's already contained in the memory of the Jupyter Notebook under variable `X`. `Y` contains the label of each handwritten digit, i.e. the number, or the class. [Documentation on sklearn PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) \n",
    "1. Plot the total variance vs the number of PC. **Hint**: variable `explained_variance_ratio_` may be useful.\n",
    "2. Use the first two component to represent MNIST data set in two dimensions on a scatter plot. \n",
    "3. Show the image of first first principal eigenvector. **Hint**: variable `components_` and function `reshape()` might be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multidimensional Scaling (MDS) <a id='l_mds'></a>\n",
    "\n",
    "#### Metric MDS\n",
    "Setup:\n",
    "- Suppose one has $p$ samples of N-dimensional data points, $x_i\\in\\mathbb{R}^N$\n",
    "- Store these samples rowwise as $X\\in\\mathbb{R}^{p\\,\\times\\,N}$\n",
    "- We call this the original data matrix, or simply the data\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the data space (high dim)\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the latent space (low dim)\n",
    "\n",
    "Goal:\n",
    "- Given N-dim data $X$, a metric $d(\\cdot,\\cdot)$ on $\\mathbb{R}^N$, a target dimension $k<N$, and a metric $g(\\cdot,\\cdot)$ on $\\mathbb{R}^k$\n",
    "- Find an embedding $Y\\in\\mathbb{R}^{k\\,\\times\\,p}$ (i.e. a $y_i\\in\\mathbb{R}^k$ for each $x_i\\in\\mathbb{R}^N$) such that distances $d_{ij}$, $g_{ij}$ are preserved between representations\n",
    "\n",
    "Objective function: $$Y^\\ast=\\operatorname*{arg\\,min}_Y {\\sum_{i<j}{\\left|d_{ij}\\left(X\\right)-g_{ij}\\left(Y\\right)\\right|}}$$\n",
    "\n",
    "Let's not invent the wheel and use MDS implementation in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "# compute MDS embedding (2D)\n",
    "# Docs: http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we predict the outcome of MDS from 2D to 2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we should find pairwise distances. \n",
    "D_compressed = pdist(X_2d, metric='euclidean') # n*(n-1)/2 size\n",
    "D = squareform(D_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.displayr.com/wp-content/uploads/2018/04/Distance-Matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_2d = MDS(n_components=2, dissimilarity='precomputed', n_jobs=-1 ).fit_transform(D)\n",
    "plt.scatter(mds_2d[:,0], mds_2d[:,1], color='red')\n",
    "plt.ylim((-6,6))\n",
    "plt.xlim((-8,8))\n",
    "plt.xlabel('MDS axis 1')\n",
    "plt.ylabel('MDS axis 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restores original coordinates up to a flip..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about high dimensional data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_X = MDS(n_components=2, dissimilarity='euclidean', n_jobs=4).fit_transform(X) ## distances will be computed\n",
    "\n",
    "plt.scatter(mds_X[:,0], mds_X[:,1], marker='.')\n",
    "plt.xlabel('MDS axis 1')\n",
    "plt.ylabel('MDS axis 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we get as an output of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in set(Y):\n",
    "    mask = Y==label\n",
    "    plt.scatter(mds_X[:,0][mask], mds_X[:,1][mask], marker = '.', label = label)\n",
    "plt.legend()\n",
    "plt.xlabel('MDS axis 1')\n",
    "plt.ylabel('MDS axis 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.1. \n",
    "Find the relative coordinates (sketch of the map) of the European cities knowing only pairwise distances between them. **Remember that MDS embedding may require rotation and/or horizontal and vertical flip.** You should get relative locations similar to these:\n",
    "![](pics/europe.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Pairwise distance between European cities\n",
    "url = 'https://media.githubusercontent.com/media/neurospin/pystatsml/master/datasets/eurodist.csv'\n",
    "df = pd.read_csv(url)\n",
    "print(df.iloc[:7, :7])\n",
    "print()\n",
    "## Array with cities:\n",
    "city = np.array(df[\"city\"])\n",
    "\n",
    "## Squareform distance matrix\n",
    "D = np.array(df.iloc[:, 1:]) \n",
    "print(D.shape)\n",
    "print()\n",
    "print(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Other dimensionality reduction techniques: t-SNE, UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [t-SNE](https://lvdmaaten.github.io/tsne/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_2d = TSNE(n_components=2).fit_transform(X)\n",
    "\n",
    "plt.scatter(tsne_2d[:,0], tsne_2d[:,1], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in set(Y):\n",
    "    mask = Y==label\n",
    "    plt.scatter(tsne_2d[:,0][mask], tsne_2d[:,1][mask], marker = '.', label = label)   #, color =  label + 1) #, label = str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [UMAP](https://github.com/lmcinnes/umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_2d = umap.UMAP(n_components=2).fit_transform(X)\n",
    "\n",
    "plt.scatter(umap_2d[:,0], umap_2d[:,1], marker='.', color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in set(Y):\n",
    "    mask = Y==label\n",
    "    plt.scatter(umap_2d[:,0][mask], umap_2d[:,1][mask], marker = '.', label = label)   #, color =  label + 1) #, label = str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
